{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifing last names with character-level RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "`wget https://github.com/hunkim/PyTorchZeroToAll/blob/master/data/names_train.csv.gz`\n",
    "\n",
    "`wget https://github.com/hunkim/PyTorchZeroToAll/blob/master/data/names_test.csv.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data2/yinterian/name_dataset/names_test.csv'),\n",
       " PosixPath('/data2/yinterian/name_dataset/names_train.csv.gz'),\n",
       " PosixPath('/data2/yinterian/name_dataset/names_train.csv'),\n",
       " PosixPath('/data2/yinterian/name_dataset/names_test.csv.gz')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"/data2/yinterian/name_dataset/\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Adsit\",\"Czech\"\r",
      "\r\n",
      "\"Ajdrna\",\"Czech\"\r",
      "\r\n",
      "\"Antonowitsch\",\"Czech\"\r",
      "\r\n",
      "\"Antonowitz\",\"Czech\"\r",
      "\r\n",
      "\"Ballalatak\",\"Czech\"\r",
      "\r\n",
      "\"Ballaltick\",\"Czech\"\r",
      "\r\n",
      "\"Bastl\",\"Czech\"\r",
      "\r\n",
      "\"Baroch\",\"Czech\"\r",
      "\r\n",
      "\"Betlach\",\"Czech\"\r",
      "\r\n",
      "\"Biganska\",\"Czech\"\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head /data2/yinterian/name_dataset/names_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH/\"names_train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', \"'\", ',', 'A', 'B', 'C', 'D', 'E', 'F', 'G']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting a vocabulary of characters\n",
    "letters = [list(l) for l in df[0].values]\n",
    "vocab = sorted(list(set(np.concatenate(np.array(letters)))))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2id = {key:i for i, key in enumerate(vocab)}\n",
    "vocab2id[\" \"] # I am going to use 0 to pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 0,\n",
       " 'Chinese': 1,\n",
       " 'Czech': 2,\n",
       " 'Dutch': 3,\n",
       " 'English': 4,\n",
       " 'French': 5,\n",
       " 'German': 6,\n",
       " 'Greek': 7,\n",
       " 'Irish': 8,\n",
       " 'Italian': 9,\n",
       " 'Japanese': 10,\n",
       " 'Korean': 11,\n",
       " 'Polish': 12,\n",
       " 'Portuguese': 13,\n",
       " 'Russian': 14,\n",
       " 'Scottish': 15,\n",
       " 'Spanish': 16,\n",
       " 'Vietnamese': 17}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(df[1].unique())\n",
    "label2id = {key:i for i, key in enumerate(labels)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(x, seq_len=15, vocab2id=vocab2id):\n",
    "    x = list(x)\n",
    "    x = np.array([vocab2id[k] for k in x])\n",
    "    z = np.zeros(seq_len, dtype=np.int32)\n",
    "    n = min(seq_len, x.shape[0])\n",
    "    z[seq_len - n:] = x[0:n]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 29, 29, 30, 30, 30],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pad_seq(\"aabbb\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "def seq2matrix(x, vocab_len=55):\n",
    "    z = np.zeros((x.shape[0], vocab_len))\n",
    "    z[np.arange(len(x)), x] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, path, vocab2id, label2id, seq_len=15, vocab_len=55):\n",
    "        self.df = pd.read_csv(path, header=None)\n",
    "        self.label2id = label2id\n",
    "        self.vocab2id = vocab2id\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_len = vocab_len \n",
    "        self.x = df[0].values\n",
    "        self.y = [self.label2id[l] for l in df[1].values]\n",
    "        self.vocab2id = vocab2id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = pad_seq(self.x[idx], self.seq_len, self.vocab2id)\n",
    "        x = seq2matrix(x, self.vocab_len)\n",
    "        return x, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = NameDataset(PATH/\"names_train.csv\", vocab2id, label2id)\n",
    "test = NameDataset(PATH/\"names_test.csv\", vocab2id, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "n = len(test)\n",
    "train_dl = DataLoader(train, batch_size=batch_size)\n",
    "test_dl = DataLoader(test, batch_size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13374, 13374)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 55) 2\n"
     ]
    }
   ],
   "source": [
    "x,y = train[0]\n",
    "print(x.shape,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.linear_h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        combined = torch.cat((x, hidden), 1)\n",
    "        hidden = torch.tanh(self.linear_i2h(combined))\n",
    "        output = self.linear_h2o(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, bash_size):\n",
    "        return torch.zeros(bash_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 55\n",
    "hidden_size = 100\n",
    "n_classes = 18\n",
    "model = CharRNN(vocab_size, hidden_size, n_classes).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2000, 15, 55]), torch.Size([2000]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = x.shape[0]\n",
    "h = model.initHidden(batch).cuda()\n",
    "x = x.cuda().float()\n",
    "y = y.cuda().long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 155])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x[:,0], h), 1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ei in range(x.shape[1]):\n",
    "    x_t, h = model(x[:,ei], h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9295, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(x_t, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 55\n",
    "hidden_size = 100\n",
    "n_classes = 18\n",
    "model = CharRNN(vocab_size, hidden_size, n_classes).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr = 0.01, wd = 0.00001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optim, train_dl):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x, y in train_dl:\n",
    "        batch = x.shape[0]\n",
    "        h = model.initHidden(batch).cuda()\n",
    "        loss = 0\n",
    "        x = x.cuda().float()\n",
    "        y = y.cuda().long()\n",
    "        \n",
    "        for t in range(x.shape[1]):\n",
    "            out, h = model(x[:,t], h)\n",
    "        \n",
    "        loss = F.cross_entropy(out, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += batch\n",
    "        sum_loss += batch*(loss.item())\n",
    "    return sum_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_dl):\n",
    "    model.eval()\n",
    "    x, y = next(iter(test_dl))\n",
    "    x = x.cuda().float()\n",
    "    y = y.cuda().long()\n",
    "    N = x.shape[0]\n",
    "    h = model.initHidden(N).cuda()\n",
    "    for t in range(x.shape[1]):\n",
    "        out, h = model(x[:,t], h)\n",
    "    loss = F.cross_entropy(out, y)\n",
    "    _, pred = torch.max(out, 1)\n",
    "    acc = pred.eq(y).sum().float()/N\n",
    "    print(\"test loss %.3f and accuracy %.3f\" % (loss.item(), acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 55\n",
    "hidden_size = 80\n",
    "n_classes = 18\n",
    "model = CharRNN(vocab_size, hidden_size, n_classes).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, lr, epochs=20):\n",
    "    optim = get_optimizer(model, lr =lr, wd = 0.0)\n",
    "    for i in range(epochs):\n",
    "        loss = train(model, optim, train_dl)\n",
    "        if i%5 == 1: print(\"train loss %.3f\" % loss)\n",
    "    predict(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.165\n",
      "train loss 1.891\n",
      "train loss 1.633\n",
      "train loss 1.576\n",
      "test loss 1.348 and accuracy 0.567\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, lr=0.01, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.354\n",
      "train loss 1.318\n",
      "train loss 1.297\n",
      "train loss 1.272\n",
      "test loss 1.238 and accuracy 0.611\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, lr=0.001, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.242\n",
      "train loss 1.216\n",
      "train loss 1.197\n",
      "train loss 1.176\n",
      "test loss 1.143 and accuracy 0.648\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, lr=0.001, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.149\n",
      "train loss 1.132\n",
      "train loss 1.119\n",
      "train loss 1.103\n",
      "train loss 1.088\n",
      "train loss 1.072\n",
      "train loss 1.058\n",
      "train loss 1.043\n",
      "test loss 1.014 and accuracy 0.691\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, lr=0.001, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.064\n",
      "train loss 1.018\n",
      "train loss 1.005\n",
      "train loss 0.995\n",
      "train loss 0.984\n",
      "train loss 0.973\n",
      "train loss 0.963\n",
      "train loss 0.954\n",
      "test loss 0.928 and accuracy 0.724\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, lr=0.001, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.952\n",
      "train loss 0.935\n",
      "train loss 0.927\n",
      "train loss 0.920\n",
      "train loss 0.912\n",
      "train loss 0.904\n",
      "train loss 0.897\n",
      "train loss 0.889\n",
      "test loss 0.866 and accuracy 0.742\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, lr=0.001, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.882\n",
      "train loss 0.873\n",
      "train loss 0.870\n",
      "train loss 0.865\n",
      "train loss 0.859\n",
      "train loss 0.853\n",
      "train loss 0.847\n",
      "train loss 0.841\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, lr=0.001, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with character embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDatasetEmb(Dataset):\n",
    "    def __init__(self, path, vocab2id, label2id, seq_len=15, vocab_len=55):\n",
    "        self.df = pd.read_csv(path, header=None)\n",
    "        self.label2id = label2id\n",
    "        self.vocab2id = vocab2id\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_len = vocab_len \n",
    "        self.x = df[0].values\n",
    "        self.y = [self.label2id[l] for l in df[1].values]\n",
    "        self.vocab2id = vocab2id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = pad_seq(self.x[idx], self.seq_len, self.vocab2id)\n",
    "        return x, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2 = NameDatasetEmb(PATH/\"names_train.csv\", vocab2id, label2id)\n",
    "test_2 = NameDatasetEmb(PATH/\"names_test.csv\", vocab2id, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "n = len(test)\n",
    "train_2_dl = DataLoader(train_2, batch_size=batch_size)\n",
    "test_2_dl = DataLoader(test_2, batch_size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEmbRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, output_size):\n",
    "        super(CharEmbRNN, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_i2h = nn.Linear(emb_size + hidden_size, hidden_size)\n",
    "        self.linear_h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = x.long()\n",
    "        x = self.emb(x)\n",
    "        combined = torch.cat((x, hidden), 1)\n",
    "        hidden = F.tanh(self.linear_i2h(combined))\n",
    "        output = self.linear_h2o(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, bash_size):\n",
    "        return torch.zeros(bash_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, epochs, lr=0.01, wd=0.0):\n",
    "    optim = get_optimizer(model, lr = lr, wd = wd)\n",
    "    for i in range(epochs):\n",
    "        loss = train(model, optim, train_2_dl)\n",
    "        if i%5 == 1: print(\"train loss %.3f\" % loss)\n",
    "    predict(model, test_2_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 55\n",
    "emb_size = 30\n",
    "hidden_size = 80\n",
    "n_classes = 18\n",
    "model = CharEmbRNN(vocab_size, emb_size, hidden_size, n_classes).cuda()\n",
    "optim = get_optimizer(model, lr =0.01, wd = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 5.018\n",
      "train loss 1.601\n",
      "train loss 1.621\n",
      "train loss 1.369\n",
      "train loss 1.287\n",
      "train loss 1.216\n",
      "train loss 1.154\n",
      "train loss 1.099\n",
      "test loss 0.991 and accuracy 0.692\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, epochs=40, lr=0.01, wd=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.255\n",
      "train loss 1.047\n",
      "train loss 0.959\n",
      "train loss 0.897\n",
      "test loss 0.802 and accuracy 0.762\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, epochs=20, lr=0.01, wd=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.805\n",
      "train loss 0.793\n",
      "train loss 0.787\n",
      "train loss 0.779\n",
      "test loss 0.767 and accuracy 0.771\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, epochs=20, lr=0.001, wd=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.773\n",
      "train loss 0.764\n",
      "train loss 0.759\n",
      "train loss 0.753\n",
      "train loss 0.746\n",
      "train loss 0.740\n",
      "train loss 0.733\n",
      "train loss 0.727\n",
      "test loss 0.715 and accuracy 0.786\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, epochs=40, lr=0.001, wd=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.379\n",
      "train loss 0.372\n",
      "train loss 0.367\n",
      "train loss 0.362\n",
      "test loss 0.354 and accuracy 0.891\n",
      "train loss 0.359\n",
      "train loss 0.352\n",
      "train loss 0.348\n",
      "train loss 0.343\n",
      "test loss 0.335 and accuracy 0.898\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, epochs=20, lr=0.001, wd=0.0)\n",
    "train_loop(model, epochs=20, lr=0.001, wd=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.340\n",
      "train loss 0.333\n",
      "train loss 0.329\n",
      "train loss 0.325\n",
      "test loss 0.317 and accuracy 0.904\n",
      "train loss 0.322\n",
      "train loss 0.316\n",
      "train loss 0.312\n",
      "train loss 0.307\n",
      "test loss 0.300 and accuracy 0.911\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, epochs=20, lr=0.001, wd=0.0)\n",
    "train_loop(model, epochs=20, lr=0.001, wd=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.306\n",
      "train loss 0.299\n",
      "train loss 0.295\n",
      "train loss 0.291\n",
      "test loss 0.284 and accuracy 0.917\n",
      "train loss 0.291\n",
      "train loss 0.283\n",
      "train loss 0.280\n",
      "train loss 0.276\n",
      "test loss 0.269 and accuracy 0.922\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, epochs=20, lr=0.001, wd=0.0)\n",
    "train_loop(model, epochs=20, lr=0.001, wd=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.276\n",
      "train loss 0.269\n",
      "train loss 0.265\n",
      "train loss 0.261\n",
      "test loss 0.255 and accuracy 0.927\n",
      "train loss 0.263\n",
      "train loss 0.255\n",
      "train loss 0.251\n",
      "train loss 0.248\n",
      "test loss 0.242 and accuracy 0.931\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, epochs=20, lr=0.001, wd=0.0)\n",
    "train_loop(model, epochs=20, lr=0.001, wd=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Change the first model to learn a character language model that generates last names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "This notebook is a modified version of this tutorial\n",
    "http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html. Here I implement vanilla RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
